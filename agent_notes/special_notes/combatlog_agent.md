Great. I’ll write a detailed technical specification for implementing a multi-agent system using the OpenAI Agents API (March 2025 release) and PandasAI to analyze SMITE 2 combat logs. The system will run in a local Streamlit application, operate over a session-local SQLite database, and include a set of agents that can interact with PandasAI, run SQL, generate charts, and produce complete analyst-ready answers.

I’ll design the architecture to support multiple agents (e.g. orchestrator, data-frame specific specialists, evaluator), describe the use of function-calling to run PandasAI or SQL logic, and include example workflows, tool setup, and the interaction model inside Streamlit. I will also specify caching/logging strategies, visualization tools, and safe sandboxed handling of dynamic user queries.

I’ll get started and let you know when the spec is ready for review.

# Technical Specification: Streamlit Multi‑Agent SMITE 2 Combat Log Analysis System

## Overview  
This specification describes a **Streamlit-integrated multi-agent chatbot application** for analyzing SMITE 2 combat log data. The system leverages OpenAI’s March 2025 **Agents API** (Responses API + Agents SDK) and the **PandasAI** library to interpret natural language questions and generate answers with combined text, tables, and charts. Users upload a SMITE 2 combat log (JSON) which is parsed into a **SQLite database**. Each user query triggers an **Orchestrator Agent** that coordinates specialized sub-agents and tools to produce insightful answers for esports analysts/coaches. The application runs self-contained in Streamlit, meaning all data (SQLite DB) and computation is local to the session (no external calls except OpenAI APIs). Responses are conversational yet data-backed, providing interactive analytics on match data such as combat events, timelines, and player statistics.

## System Architecture  
The system architecture is a **multi-agent orchestration** built on OpenAI’s Agents SDK, which simplifies multi-step workflows by allowing LLM-based agents to use tools and hand off tasks. Key components include: 

- **Orchestrator Agent (Triage Agent)** – an LLM agent that receives user questions and decomposes them into subtasks. It decides which specialized agent or tool should handle each subtask (via **handoff** or function call) and assembles the final answer. This agent maintains the overall conversation state and ensures the answer addresses the user’s query comprehensively.
- **Dataframe Specialist Agents** – dedicated LLM agents focused on specific domains of the data. For example:  
  - *CombatEventsAgent*: Expert in the `combat_events` table (damage, kills, healing events).  
  - *TimelineAgent*: Expert in `timeline_events` (chronological match events, objectives, key moments).  
  - *PlayerStatsAgent*: Expert in player performance stats (possibly aggregated stats per player).  
  Each specialist agent is configured with knowledge of its table’s schema and semantics, enabling it to interpret questions and interact with data for that domain. These agents have focused instructions (system prompts) describing the table (columns, sample values, meaning) and are equipped with tools for data retrieval/analysis. For instance, the CombatEventsAgent can retrieve damage logs or compute DPS, the TimelineAgent can extract sequences of objectives or turning points, etc. The specialization ensures that domain-specific jargon or context (e.g. “kills”, “gold lead”, “DPS”) is well understood by the agent.  
- **Shared Tools (Function Calls)** – A set of internal Python functions exposed to the agents via OpenAI’s function-calling interface. These tools allow agents to perform actions like querying the database or running PandasAI. The Agents SDK converts Python function definitions into JSON schemas that the LLM can use. Tools are **modular capabilities** that any agent can invoke when authorized:
  - `run_sql_query(query: str)` – Execute a SQL query on the SQLite combat log database (read-only). Used for direct data access or aggregation via SQL.
  - `run_pandasai_prompt(prompt: str, tables: List[str])` – Run a natural language analysis prompt using PandasAI on specified dataframes (derived from tables). Used for complex analysis or multi-table operations via an LLM-generated pandas workflow.
  - `generate_chart(data: Any, chart_type: str, **kwargs)` – Produce a chart (Matplotlib or Plotly) from given data and return a reference (file path or figure object) for display. Used to visualize results (bar charts, line plots, etc.).
  - `call_internal_function(name: str, args: dict)` – (Optional) Invoke a domain-specific Python function from the codebase for calculations not easily done via SQL/Pandas (e.g., a function to calculate a custom metric). This allows the agents to utilize any pre-built analytics routines.  
- **SQLite Database (SMITE Log Data)** – The structured data back-end, created by parsing the uploaded log. Core tables include `matches` (match metadata), `players` (player info), and various event tables such as `combat_events`, `reward_events`, `item_events`, `player_events`, and `timeline_events`. Support tables like `items` and `abilities` provide game context. This relational schema captures all combat interactions, item buys, objectives, and timeline of the match. The multi-agent system leverages this structured data for analysis (either by direct SQL or by loading into pandas DataFrames). Each session is scoped to one match’s DB, ensuring agents only reference relevant data.  
- **In-Memory DataFrames & Shared State** – For PandasAI operations, the system may load certain tables into pandas DataFrames. For example, on session start, the application can read key tables (`combat_events`, `timeline_events`, `players`, etc.) into memory. These DataFrames are stored in a shared dictionary or **Session State** so that all agents/tools can access them. This acts as shared memory. The Orchestrator can pass references (table names or DataFrame keys) to tools instead of raw data to keep function call payloads small. Intermediate results (like a filtered DataFrame or an aggregated result set) can also be stored in memory or as a **temporary SQLite table** if needed for subsequent steps, without altering the original data. By centralizing data access, all agents operate on a consistent view of the log data.

**Interaction flow:** The user’s question goes to the Orchestrator Agent. The Orchestrator (an LLM via the Responses API) interprets the query and decides on a plan. It might call a tool directly or **handoff** to a specialist agent. For simple queries, the Orchestrator might suffice; for complex ones, it delegates subtasks. Each agent’s LLM reasoning is augmented by these function calls (tools) that fetch or compute actual data, which is then fed back into the agent’s context for further reasoning (the **Agent Loop**). The Agents SDK automates much of this loop: the agent will attempt an answer, realize it needs data, invoke a function, get the result, and incorporate it, iterating until completion. Specialized agents can likewise call back to the Orchestrator or other tools if a subtask needs broader context. All agents operate under an **“expert” persona** as per instructions and use shared data/state, enabling collaborative problem-solving.

## Agents and Orchestration Details

### Orchestrator Agent  
**Role:** The Orchestrator (akin to a *Triage Agent*) is the entry point for all user queries. It is configured with system instructions like: *“You are an AI assistant that answers questions about SMITE 2 match logs by coordinating specialized agents and tools. You break queries into parts, use the appropriate data tool or agent, and compile a clear, helpful answer with evidence (tables/charts) as needed.”* This agent does not directly contain domain knowledge but **routes tasks intelligently**. It uses OpenAI’s handoff mechanism to delegate when another agent is better suited. For instance, if the question is *“Which player dealt the most damage overall and when did they peak?”*, the Orchestrator might identify two subtasks: (1) a **combat stats query** (“who dealt most damage?”) and (2) a **timeline query** (“when was their peak damage moment?”). It could hand off the first to CombatEventsAgent and the second to TimelineAgent, then combine the results.

**Capabilities:** The Orchestrator has access to all function tools and all specialized agents via the Agents SDK. Its `tools=[run_sql_query, run_pandasai_prompt, generate_chart, ...]` and `handoffs=[CombatEventsAgent, TimelineAgent, PlayerStatsAgent, ...]`. When confronted with a user prompt, it uses chain-of-thought reasoning internally to decide on a course of action:
- If the query explicitly or implicitly references a specific data domain (detected via keywords or using a small prompt to itself), it will delegate to the corresponding agent. *E.g.* if the question mentions “timeline” or “sequence of events”, use TimelineAgent; if it mentions “damage” or “kills”, involve CombatEventsAgent; if it’s about “player performance” or aggregate stats, use PlayerStatsAgent.
- If the query is broad or involves correlation of multiple data sources (e.g. “How did gold lead correlate with team fights?” involves economy and combat), the Orchestrator may either handle it by calling multiple tools itself or orchestrate a multi-agent dialogue (have one agent generate some data which is then given to another). The orchestrator keeps track of what information each sub-agent can provide and may merge their outputs. 
- For straightforward data retrieval or computation, the Orchestrator might bypass sub-agents and call a tool directly. For example, a question like “What is the final kill count for each team?” could be answered by a single SQL query; the Orchestrator can directly call `run_sql_query` with a GROUP BY on `combat_events` (kill events per team) and then format the result.
- The Orchestrator also ensures the final presentation is polished. It might call `generate_chart` after obtaining raw data to produce a visualization, or format a Pandas DataFrame result into a nice table. It adds explanatory text as needed for context.

**Memory:** The Orchestrator Agent maintains conversation state. Streamlit can store the conversation (e.g. previous Q&A pairs) and pass it as part of the context (system or assistant messages) so the agent can handle follow-up questions. For example, if a user asks *“Who had the most kills?”* and then follows with *“Show me their damage over time”*, the Orchestrator should recall that “their” refers to the top killer from the prior answer. This could be achieved by storing the identity (player name) from the first answer in session state and/or in the conversation context given to the LLM for the second query. The Agents SDK’s tracing can log these interactions for debugging but the conversation memory itself can be maintained by appending messages to the LLM prompt (with care for token limits).

### Dataframe-Specialized Agents  
Each specialized agent is an LLM (via Responses API) with a focused scope and tools restricted to its domain. They function like “consultants” that the Orchestrator can engage for detailed analysis. All share the same underlying model (e.g. GPT-4) but differ in instructions and accessible data. The main specialized agents include:

- **CombatEventsAgent** – Instructions: *“You are a Combat Log Analyst AI specialized in combat events. You have deep knowledge of the `combat_events` table which logs damage, kills, and healing. Columns include attacker, target, ability, damage amount, timestamp, etc. Use tools to retrieve or analyze combat_events data to answer questions about damage, kills, abilities, and fight outcomes. Provide concise, factual answers with data.”*  
  **Tools:** `run_sql_query` and `run_pandasai_prompt` (scoped to combat_events DataFrame) and possibly `generate_chart`.  
  **Behavior:** This agent can handle questions like “Who dealt the highest burst damage in a single event?”, “List all kills by player X”, or “Compare average damage taken by tanks vs mages”. It might use SQL for simple filters/aggregations (e.g. sum of damage per player) or PandasAI for more complex analysis (e.g. time-series analysis of damage). The agent may also produce a quick plot (damage over time) by calling `generate_chart` if needed or simply return data to the orchestrator for visualization. It understands game-specific context (e.g. the difference between damage types or how to interpret ability usage). If a query exceeds its scope (e.g. “Which team won the match?” is outside combat_events alone), it can either respond that the question is out of scope or call back to the orchestrator (through a tool or by finishing with a handoff suggestion).  

- **TimelineAgent** – Instructions: *“You are a Timeline Analyst AI specialized in match timeline events. You focus on the `timeline_events` table, which contains a chronological sequence of important events (objectives taken, kills, towers destroyed, etc.) with timestamps and importance ratings. Use tools to extract sequences or analyze timing of events.”*  
  **Tools:** Similarly, it can query the timeline table or use PandasAI to derive insights (like periods of dominance, or time between objectives).  
  **Behavior:** Handles queries like “What were the major turning points of the match?”, “How quickly was the first objective taken?”, “Plot the kill timeline for each team.” This agent might directly query for events with high importance rating or certain types (e.g. find when the Gold Fury was killed). It can return a list of events or even a narrative summary of the timeline (since timeline_events often have description text). If asked for a chart (like kills over time), it could prepare data (e.g. cumulative kills for each team vs time) and either call `generate_chart` or return the data for the orchestrator to chart. It is aware of chronological context and can reference earlier/later events.

- **PlayerStatsAgent** – Instructions: *“You are a Player Stats Analyst AI specialized in aggregated player data. You utilize the `players` table and any derived stats tables (like `player_stats` if available) which contain end-of-match statistics (kills, deaths, assists, damage done, gold earned, etc.) for each player. You answer questions comparing player performances, rankings, and efficiency.”*  
  **Tools:** This agent might rely heavily on `run_sql_query` because aggregated stats are often a single-row per player – straightforward to query. It can also use PandasAI if a derived calculation is needed (e.g., “calculate each player’s KDA and rank them”).  
  **Behavior:** Answers queries such as “Who was the MVP based on stats?”, “Compare the gold per minute of each player”, “Which role had the highest average damage?”. It likely performs grouping or sorting queries. For more nuanced questions (like “who improved the most compared to their average?” if multiple matches were present), PandasAI could be used to combine current match data with reference data (though each session is one match, so cross-session comparison might not apply unless we load historical data too).

- **(Optional) Other Agents** – Depending on needs, more specialized agents could be defined, such as an *ItemsAgent* for item purchase analysis (using `item_events` table to see build paths, etc.) or an *ObjectivesAgent* (though those are in timeline events). However, the above three cover the primary analysis areas: combat, timeline, and player performance. 

Each agent is created via the Agents SDK, for example: 
```python
combat_agent = Agent(
    name="CombatEventsAgent",
    instructions="You are a combat log expert ... (as above)",
    tools=[run_sql_query, run_pandasai_prompt, generate_chart]
)
```
(similarly for others, each getting the appropriate tools and knowledge). These agents do not run continuously; they are invoked by the Orchestrator via handoff when needed. The Agents SDK’s **handoff mechanism** allows the Orchestrator to transfer control to one of these agents mid-conversation. In practice, the Orchestrator’s policy (learned or via prompt engineering) will decide when to yield to a sub-agent. For example, the Orchestrator might internally respond with a special action like: `<<handoff to CombatEventsAgent>>` along with the user’s question or a reformulated subtask, which the SDK intercepts to switch the active agent.

**Shared Context & Memory:** When a handoff occurs, the specialized agent should receive context of the question and any necessary intermediate data. The system can accomplish this by packaging relevant info into the handoff prompt. For instance, if the Orchestrator already fetched some data or identified an entity (like a player name), it can include that in the message to the sub-agent: *“User asked: 'Show me their damage over time.' (Note: 'their' refers to PlayerX)”*. All agents have access to the global data (via tools) but not to each other’s internal chain-of-thought. So the Orchestrator must explicitly pass any info needed. After a specialist agent finishes its task (producing some result or answer), control returns to the Orchestrator agent (either automatically after the sub-agent provides an answer, or via another handoff back). The Orchestrator then uses that result in the broader context of the user’s query. 

**Guardrails:** To ensure safety and data integrity, each agent/tool can have guardrails. For example, the SQL tool can be wrapped to **reject any non-SELECT query** or any command that attempts to modify data. The Agents SDK supports input validation guardrails. We define the SQL tool such that if the `query` string matches a forbidden pattern (UPDATE/DELETE/DROP), it returns an error or empty result, and the agent knows not to attempt destructive actions. Similarly, PandasAI prompts should be phrased to avoid arbitrary code execution beyond data analysis. (PandasAI by design generates Pandas code; we will sandbox its execution environment to just the data frames). These measures prevent agents from causing side effects or accessing anything outside the provided data.

## Tools and Function Schema Definitions

We expose internal functionality as tools to the agents using OpenAI’s function calling interface. Below are the main tools with their intended schema:

- **`run_sql_query`** – *Execute an SQL query on the combat log database.*  
  **Parameters:** `{ "query": {"type": "string", "description": "SQL SELECT query or CREATE TEMP TABLE query to run on the SQLite log database"} }`.  
  **Operation:** When the agent calls this, the system will run the provided SQL against the SQLite database (connection is already opened to the session’s DB file). If the query is a `SELECT`, it returns the resulting rows. The return format can be a Python object (e.g. list of tuples or a Pandas DataFrame). The Agents SDK will serialize it to a friendly format for the LLM. Most likely, we’ll convert the result to a small JSON (for example, a list of dictionaries for each row, or an HTML/Markdown table if returning to be displayed). If the result set is large, the tool may truncate or summarize results (e.g. top N rows) and store the full result in a temporary table or memory for further reference. This tool also supports `CREATE TEMPORARY TABLE AS SELECT ...` if the agent wants to store intermediate results for later queries. It will **not allow** any permanent schema changes or deletions. For observability, each executed query can be logged.

  **Example Tool Definition (pseudo-code):**  
  ```python
  @function_tool
  def run_sql_query(query: str) -> str:
      """Execute a read-only SQL query on the SMITE log SQLite database and return the results."""
      # Validate query (no modifications)
      if is_dangerous(query): 
          return "ERROR: Forbidden query."
      # Execute query
      cur = db_conn.execute(query)
      rows = cur.fetchall()
      # Convert to output (markdown table or JSON string)
      return format_rows_as_markdown(rows, cur.description)
  ```  
  In the Agents SDK, the above decorator would generate the JSON schema for the function (with name `run_sql_query`, the string parameter, etc.), making it available to the agent’s LLM context. The `format_rows_as_markdown` might produce a pipe-separated table string if the agent is expected to directly output it to the user. Alternatively, we could return a Python data structure and let the Orchestrator decide how to present it.

- **`run_pandasai_prompt`** – *Use PandasAI (LLM-powered Pandas assistant) to analyze dataframes with natural language.*  
  **Parameters:** `{ "prompt": {"type": "string", "description": "Analysis question or instruction for the dataframes"}, "tables": {"type": "array", "items": {"type": "string"}, "description": "List of table names to load into Pandas dataframes for this analysis"} }`.  
  **Operation:** This tool bridges to the PandasAI library. When called, the system will:  
   1. Load the specified tables from SQLite into Pandas DataFrames (if not already cached). We maintain a dict of DataFrames, so we simply retrieve those. If multiple tables are provided, PandasAI can accept a list of DataFrames (it can reason across them).  
   2. Invoke PandasAI with the given prompt on those DataFrames. We initialize PandasAI with a suitable LLM (likely the same OpenAI model or a slightly smaller one for cost efficiency). For example:  
      ```python
      pandas_ai = PandasAI(llm=openai_api, conversational=False)
      result = pandas_ai(dataframes_list, prompt=prompt)
      ```  
      PandasAI will generate Python pandas code internally to answer the question ([Pandas-ai](https://pandasai-docs.readthedocs.io/#:~:text=makes%20Pandas%20conversational%2C%20allowing%20you,DataFrame%20containing%20only%20those%20rows)). It makes Pandas usage conversational, so the agent calling this doesn’t need to write code itself – it just asks in plain language and gets back an answer (which could be a DataFrame, a number, or a textual explanation, depending on the prompt). **Example:** If prompt = *"Calculate the average damage per minute for each player and highlight the highest."*, PandasAI might produce code to compute damage per minute and return a DataFrame or string identifying the top value.  
   3. Capture the result. If it’s a DataFrame or numpy array, we can return it as JSON or markdown table. If it’s a Matplotlib chart (PandasAI can sometimes create plots if instructed), we intercept the plot object (e.g., we may have to check `matplotlib.pyplot` state) and save it to file, then return a reference (though we prefer to handle charts via the separate tool to keep things modular). If it’s a plain text answer, return the text.  
   4. Potentially post-process the result. For example, if PandasAI returned a large DataFrame, maybe summarize or store it as temp for further querying.  
   
   **Prompt Engineering within the Tool:** We can prepend some context to the prompt given to PandasAI to make it more effective for esports analysis. For instance: *"You are an AI data analyst for esports match data. The user is interested in: {prompt}. Using the provided dataframes, perform the necessary analysis. If the result is numeric or tabular, output a pandas DataFrame or summary. If it’s an insight, explain briefly."* This primes PandasAI to produce the desired format (DataFrame or concise text). We set `conversational=False` to avoid verbose chatty answers – we want results that the calling agent can directly use (as data or factual text). PandasAI essentially serves as a computational engine (with LLM reasoning) for complex tasks that might involve multi-step pandas operations, without the Orchestrator agent itself writing code.  
   
   **Example Tool Definition:**  
   ```python
  @function_tool
  def run_pandasai_prompt(prompt: str, tables: List[str]) -> str:
      """Use PandasAI to analyze given tables with a natural language prompt."""
      dfs = [load_dataframe(name) for name in tables]
      # Optionally add schema info or context to prompt
      full_prompt = f"As an esports data analyst, {prompt}"
      result = pandas_ai(dfs, prompt=full_prompt)
      return serialize(result)
  ```  
   Here `serialize(result)` will turn the PandasAI output into a format for the agent. If `result` is a DataFrame, perhaps convert to CSV string or JSON. If it’s a matplotlib Figure, save image and return a path or HTML tag. This function gives the agents a powerful way to do multi-step data analysis through natural language – essentially it’s an agent calling another LLM (PandasAI uses an LLM under the hood) to get the answer. We will monitor for correctness and might implement checks (for example, if PandasAI returns code execution errors, we catch and possibly re-prompt with a simpler question or different approach).

- **`generate_chart`** – *Render a chart from data (using Matplotlib/Plotly).*  
  **Parameters:** We can design this tool to be flexible by accepting either a reference to data or explicit values. Two possible modes:  
   a) `{ "data": {"type": "object", "description": "Tabular data (e.g. list of records or DataFrame) to visualize"}, "chart_type": {"type": "string", "enum": ["line","bar","scatter","pie",...], "description": "Type of chart"}, "x": {"type": "string","description": "Column for x-axis"}, "y": {"type": "string","description": "Column(s) for y-axis"}, "group": {"type": "string","description": "Column for grouping (optional)"} }`.  
   b) Alternatively, `{ "table_name": {"type": "string", "description": "Name of a temp table or dataset to plot"}, "spec": {"type": "string", "description": "Plot specification or title/description"} }`.  
   The first option is more explicit but passing a large data object via JSON may be impractical. So we might implement (b): the agent can call `generate_chart` after having created a temp table or knowing a table name. For example, the agent could do: `run_sql_query("CREATE TEMP TABLE dmg_by_min AS SELECT minute, SUM(damage) as total_dmg FROM combat_events ...")` then `generate_chart(table_name="dmg_by_min", spec="line chart of total_dmg over minute")`. Our function will then retrieve that temp table into a DataFrame and plot it.  
   
  **Operation:** When called, the function will identify the dataset (either use provided data or load the named table/DataFrame from our session state). Then, using a plotting library:
   - If using **Matplotlib**: create a figure, plot according to chart_type (e.g. plt.plot for line, plt.bar for bar, etc.), set labels and title if provided in spec, save the figure to an image file (like a temporary PNG in the Streamlit static directory). Return the file path or an HTML image tag. Streamlit can then display this image in the chat. 
   - If using **Plotly**: create a Plotly figure (px.line, px.bar, etc.), and directly return the figure object. Streamlit’s front-end can detect and render a Plotly figure returned from a tool call (assuming we handle it appropriately). We might instead store the figure in session and return an identifier.  
   The tool ensures charts are formatted nicely (with legends, axis labels if possible) and perhaps limited in size (e.g., if data has too many points or categories, we might automatically take top N to keep chart readable).
   
   **Example Tool Definition:**  
   ```python
  @function_tool
  def generate_chart(table_name: str, spec: str) -> str:
      """Generate a chart from the specified data table based on the description."""
      df = load_dataframe(table_name)
      fig_path = create_chart_from_spec(df, spec)  # our internal helper
      return fig_path  # or an HTML <img> tag referencing this path
  ```  
   The `spec` might be parsed by a simple heuristic or a regex. For instance, if `spec` contains "line" and "over time", we do a line plot with x sorted by time; if "bar chart of X by Y", we do a bar plot, etc. We could also allow the agent to specify `chart_type`, `x`, `y` explicitly via separate parameters as noted. In that case, the agent has more direct control: e.g., `generate_chart(chart_type="bar", x="player_name", y="kills", data=[...])`. However, passing the data directly might exceed message size limits, so using a reference is preferred. 

   **Usage:** Typically the Orchestrator agent will call `generate_chart` after obtaining data. A specialist agent could too, but often they might hand back data to Orchestrator and let it decide on visualization. The final output to the user will embed the chart image or render the Plotly chart. This tool helps fulfill the requirement of presenting **visualizations** as part of answers, making insights clearer (for example, a gold lead graph, a bar chart of damage by player, etc., as requested by analysts).

- **`call_internal_function`** – *Invoke a custom Python function from the codebase.*  
  (This is a generic mechanism to extend the agent’s capabilities by exposing specific functions not covered by SQL/PandasAI. For instance, if the parser code has a function `calculate_teamfights(combat_events)`, we could register it.)  
  **Parameters:** `{ "name": "string", "args": "object" }` where `name` corresponds to a known function that the agent is allowed to call, and `args` is a JSON of arguments. We would maintain a lookup of safe functions (like `get_teamfight_stats`, `predict_outcome` (if any ML models), etc.).  
  **Operation:** Find the function by name and execute with provided args, return result (serialized).  
  **Usage:** The agent should use this sparingly – mainly if the user asks something we have a direct utility for. For example, if there’s a function to compute a player’s performance score, the agent can use it rather than re-implement via Pandas. We will document available internal functions to the agent in its prompt (or the tool description). This is an advanced extension point and can be omitted if not needed, but it future-proofs the system.

All function tools are **registered with the Agents SDK**, which automatically generates their JSON schemas (parameters types, etc.) for inclusion in the model’s prompt. The **Orchestrator’s prompt** will contain a listing of these tools with descriptions, so it knows what actions it can perform. For example, the prompt snippet might read: *“You have access to the following tools: `run_sql_query` – execute SQL and get data; `run_pandasai_prompt` – ask an AI to analyze data; `generate_chart` – create a chart image from data.”* along with usage examples. This way, the LLM can decide to call these functions by name with arguments, per OpenAI’s function calling protocol. Each specialized agent’s prompt will similarly list the tools it has access to (which may be a subset of the global tools). 

## Orchestration Workflow (Agent-to-Agent Delegation and Tool Use)

This section outlines how the system processes a user query step by step, using pseudocode and examples to illustrate agent delegation and tool invocation:

1. **User Input via Streamlit:** The user enters a question in the Streamlit app (e.g. *“Which player had the highest DPS, and show a chart of their damage over time.”*). When submitted, the app calls the Orchestrator Agent (e.g., `Runner.run_sync(starting_agent=orchestrator_agent, input=user_query)` in the Agents SDK).

2. **Orchestrator Interprets Query:** The Orchestrator receives the query and analyses it. Internally, the LLM (with its system prompt) will produce an initial reasoning. It might think: *“The user asks for highest DPS player (damage per second) and a damage-over-time chart for that player. I need to find DPS for all players, then find the max, then get that player’s damage over time data, then produce a chart.”* This reasoning isn’t shown to the user but guides the next steps. The Orchestrator decides on a plan:
   - It could solve it by itself using tools: e.g., run a SQL to compute DPS per player, get the top player, then run PandasAI or another SQL to get that player’s per-second or per-minute damage, then call chart.
   - Or it might invoke the CombatEventsAgent for detailed combat analysis (since DPS is combat-related), then do charting itself. Either approach is valid; for specificity, let’s say it chooses to do it via direct tools for efficiency.

3. **Tool Invocation for Data (SQL or PandasAI):** The Orchestrator’s LLM decides to call `run_sql_query` (it outputs a function call in the response). For instance, it may formulate a query: *“SELECT player_name, SUM(damage) / (MAX(timestamp) - MIN(timestamp)) AS DPS FROM combat_events GROUP BY player_name ORDER BY DPS DESC LIMIT 1.”* This query calculates DPS for each player over the match and picks the top. The function `run_sql_query` executes, returning the result (say it returns `[{"player_name": "PlayerX", "DPS": 25.4}]`). The Agents SDK supplies this result back to the Orchestrator’s LLM context, and the Orchestrator continues its reasoning with that data.  
   Now the Orchestrator knows the top DPS player is PlayerX with ~25.4 DPS. It may then either directly proceed or call another tool for the next subtask. 

4. **Delegating to Specialized Agent (if chosen):** Suppose instead the Orchestrator had decided to use the CombatEventsAgent: it would perform a handoff. The Orchestrator might include in its response something like: *“Handoff to CombatEventsAgent with task: ‘Find the player with highest DPS in the match and their DPS value.’”* The Agents SDK would then start the CombatEventsAgent, passing that as the input. The CombatEventsAgent, being an LLM with domain context, might itself call `run_sql_query` or `run_pandasai_prompt` to determine the highest DPS player. It then returns an answer (could be text like *“PlayerX had the highest DPS at 25.4.”*). The Orchestrator receives this answer from the sub-agent. In either scenario (direct tool or sub-agent), we now have “PlayerX” and their DPS.

5. **Follow-up Data Retrieval/Analysis:** Next, the question also asks for a **chart of PlayerX’s damage over time**. The Orchestrator now focuses on that. It could do: 
   - A direct SQL to get PlayerX’s damage events over time, then aggregate by time intervals. Or
   - Use PandasAI to produce a timeseries analysis for PlayerX. Or
   - Ask CombatEventsAgent (or TimelineAgent) to retrieve the data.  
   Let’s say the Orchestrator chooses a direct approach: it calls `run_sql_query` again with a query like *“SELECT minute, SUM(damage) as damage_in_min FROM combat_events WHERE player_name='PlayerX' GROUP BY minute ORDER BY minute”*. This returns a time series of damage per minute for PlayerX. Alternatively, PandasAI could be called with prompt *“For PlayerX, compute their cumulative damage over time and return a time series data.”* which might yield a DataFrame of damage vs time. Either way, we obtain the data needed for charting. The Orchestrator now has, for example, a list of (minute, damage_in_min) pairs.

6. **Chart Generation:** The Orchestrator calls the `generate_chart` tool to produce the requested visualization. For example, it may call: `generate_chart(table_name="temp_playerx_dmg", spec="PlayerX damage per minute line chart")`. If we allowed direct data passing, it might call `generate_chart(chart_type="line", x="minute", y="damage_in_min", data=that_list)`. Our `generate_chart` function executes: loads `temp_playerx_dmg` table or uses provided data, creates a line chart (time on x-axis, damage on y-axis), saves it to *playerx_damage_chart.png*, and returns the path. The Orchestrator now has a reference to the chart image.

7. **Result Composition:** The Orchestrator’s LLM now has all pieces to form the answer: the identity of the top DPS player and their DPS value, and a chart of their damage over time (with presumably some data context). The Orchestrator crafts the final answer in a user-friendly manner. It might produce something like: 

   **Text:** *“PlayerX had the highest DPS in the match, averaging **25.4 damage per second**. The chart below shows PlayerX’s damage output over time, illustrating a steady rise with significant spikes during teamfights.”*  
   **Table/Number:** (If appropriate, maybe a small table highlighting PlayerX’s total damage, DPS, etc., or just embed the numbers in text as above.)  
   **Chart:** (Embedded below the text using the image path from `generate_chart`.)

   The Orchestrator returns this as the final assistant message. In JSON terms (if using function calling under the hood, it would now return a `content` message with markdown including an image tag pointing to `playerx_damage_chart.png`). The Streamlit app then displays this nicely.

8. **Streamlit Display:** On the Streamlit side, once the Orchestrator finishes, we take the output and render it. Streamlit can interpret markdown in the chatbot response to format text and tables. For the chart image, since we have a file path, we can do `st.image("playerx_damage_chart.png")` or if we integrated the chart as a Plotly figure, use `st.plotly_chart(fig)`. The result is the user sees a chatbot response: a message with the text summary and the chart below it. If the answer included a table (say, if user asked for a table of stats), we could either show the markdown table or use `st.dataframe`. We ensure the answer formatting is clean – short paragraphs for explanations, maybe bold highlights for key numbers, etc.

9. **Iterative Improvement:** If the Orchestrator wasn’t satisfied with intermediate results, it could iterate. For example, if the SQL returned an empty result (maybe the query was wrong or data not found), the Orchestrator could try a different approach (maybe the time range assumption was wrong). Or if the PandasAI result seemed off (say it returned a weird value for DPS due to some edge case), the Orchestrator might catch that. How? Possibly via simple validations coded into tools (e.g., if DPS > some unrealistic threshold, flag) or via its own reasoning (the LLM can guess if something is unreasonable). It might then reformulate the query (maybe use a different formula for DPS or filter out downtime). This loop continues internally until the Orchestrator is confident in the answer or cannot improve further without user clarification. The Agents SDK’s agent loop helps here by feeding the latest observation (tool output) back into the model prompt so it can decide next steps.

10. **Completion:** The final answer is delivered and the Orchestrator’s turn ends. Streamlit could then allow the user to ask another question. The system retains memory of this Q&A so follow-ups can reference it. Each new question repeats the process from step 2 onward.

**Pseudocode Summary (Orchestrator logic):**  
```python
def handle_user_query(query):
    # Initial analysis (could use keywords or an LLM prompt to classify)
    domain = classify_query_domain(query)
    if domain == "combat":
        # Possibly complex combat query
        if needs_handoff(query):
            result = CombatEventsAgent.run(query)
        else:
            # direct tool usage example
            data = run_sql_query(plan_combat_sql(query))
            if not satisfactory(data):
                data = run_pandasai_prompt(plan_combat_analysis_prompt(query), tables=["combat_events"])
            result = data
    elif domain == "timeline":
        result = TimelineAgent.run(query)
    elif domain == "player_stats":
        result = PlayerStatsAgent.run(query)
    else:
        # Multi-domain or unclear, use PandasAI on multiple tables as a broad approach
        result = run_pandasai_prompt(query, tables=["combat_events","timeline_events","players"])
    # At this point, 'result' could be text or data needing formatting
    # If result includes a DataFrame or list and user likely expects a chart or table:
    if is_tabular(result):
        if user_asked_for_chart(query) or should_visualize(query, result):
            chart_path = generate_chart_from_result(result, query)
        formatted_answer = format_answer(result, chart_path)
    elif isinstance(result, str):
        formatted_answer = result  # text answer from sub-agent or PandasAI
    else:
        formatted_answer = str(result)
    return formatted_answer
```  
In this pseudocode, `classify_query_domain` might look for keywords or use a small model to decide which agent’s expertise fits. `needs_handoff` might determine if the query is complex enough to warrant an agent (perhaps if it requires reasoning beyond a single SQL). The Orchestrator tries the most straightforward path first (less cost or fewer steps), but if that fails or if the question explicitly needs deep analysis, it delegates to the LLM agents or PandasAI accordingly. After obtaining the result, it decides on visualization. `generate_chart_from_result` is a helper that calls the `generate_chart` tool with appropriate parameters derived from the data (this is essentially what we described earlier). Finally, `format_answer` composes the final Markdown, including embedding the chart if one was created (e.g. using an `![Alt text](chart_path)` in markdown or handling via Streamlit components).

Throughout this workflow, **observability** is maintained: we log each tool call and agent handoff. For example, we capture the SQL queries executed and their results, PandasAI prompts and outcomes, etc. This could be done by writing to Streamlit logs or using the Agents SDK’s trace callbacks. If enabled, a developer could inspect these logs to understand how the agent arrived at an answer (useful for debugging or improving prompts).

## Streamlit Integration and User Experience

The system is implemented as a **Streamlit app** providing a chat interface for the user to interact with the multi-agent system. Key integration points and UI/UX considerations include:

- **Session Management:** Each Streamlit session corresponds to one analysis context (one uploaded combat log file = one SQLite database). The app flow is: user uploads a log file -> backend parses it into SQLite (using the provided parser tool outside the scope of this spec) -> the database is loaded/connected and possibly key tables are cached in memory -> then the Q&A interface is presented. We ensure the Agents (Orchestrator and specialists) know which match or DB they are dealing with (e.g., via the system prompt or by using a different DB filename for each session).

- **Chat Interface:** We utilize Streamlit’s chat components (if available, e.g., `st.chat_message`) or a simpler arrangement with an input text box (`st.text_input`) and a placeholder for responses. The user enters a natural language question. When submitted, the app triggers the orchestrator logic. While the agents work (which may involve multiple API calls and some latency), we provide feedback to the user:
  - A **spinner or status messages** indicating progress. For example, Streamlit can show `st.info("Analyzing data, please wait...")` or use `with st.spinner("Running analysis..."):` around the agent call. Since the process can be multi-step, we can update the status after major milestones. E.g., after a sub-agent returns data, update the message to "Generating chart..." etc. This gives the user transparency that the question is understood and being processed in stages.
  - Optionally, **streaming responses**: If the orchestrator produces a lengthy answer, we might stream the text as it’s generated (OpenAI API supports streaming tokens). Streamlit could update the answer text gradually. However, because we often need the final answer assembled with charts and tables, it might be easier to collect the full answer then display. For now, a single final update is fine, with interim spinner.

- **Displaying Results:** Once the Orchestrator finalizes the answer, the app will display it in a nicely formatted way:
  - **Text** is shown as Markdown (using `st.markdown` or the chat message). The text will include any highlights or bullet points the agent provided. We ensure through prompt design that the agent’s text is formatted in Markdown (e.g., using bullet lists for lists of insights, bold for key values, etc., as per user’s formatting preference).
  - **Tables** can appear either as part of the markdown (the agent might have constructed a Markdown table) or as a rendered DataFrame. If the agent returns a DataFrame object or a list of dicts, we can use `st.table` or `st.dataframe` to show it with interactive features (like sorting) if appropriate. For example, an answer might include a small table of top 5 players by some metric – the agent could output that as a Markdown table, or simply return the data and let Streamlit format it. The spec requires preserving any citations in answers; if our agent includes references (not likely in this context except maybe to an internal event ID or such), we would ensure they remain formatted correctly.
  - **Charts** are displayed via Streamlit’s image or plotting functions. If `generate_chart` returned an image file path, we do `st.image(path)` to embed it. If it returned a Plotly figure, we call `st.plotly_chart(fig)`. The chart is placed in-line with the answer. We also consider adding a caption or title below the chart for clarity (the agent might include the title in the image itself or provide a caption in text).

- **Progressive Disclosure:** For very complex queries, the orchestrator might produce a lot of information. To keep the UI readable, we can break the answer into parts. For instance, first a summary sentence, then a chart, then further explanation. Streamlit can structure this by writing to the page in sections. The Orchestrator agent’s answer can be parsed if needed to separate components (though if it’s already well-formatted in markdown, we can just display it as is). Another idea: if multiple charts or tables are produced, we might use Streamlit expanders to hide detailed data unless the user wants to see it. Example: *"Detailed stats table (click to expand)"* with an expander containing a `st.dataframe`.

- **Caching and Performance:** We use Streamlit’s caching (e.g., `@st.cache_data`) for expensive operations. Loading the DataFrames from SQLite can be cached per session so that repeated queries don’t hit disk repeatedly. Also, if PandasAI is used on the same question more than once, we could cache that result to avoid duplicate API calls (though the scenario is rare unless the user repeats the same question). If the user asks a follow-up that’s identical to a previous one, we can instantly respond from memory. The multi-agent coordination itself is the slowest part (multiple OpenAI calls), so caching data helps but we also must manage the rate of LLM calls (maybe limit to needed ones only).

- **Error Handling in UI:** If something goes wrong (e.g., a tool raises an exception or the LLM fails to produce a useful response), the system should catch it and inform the user gracefully. For instance, if PandasAI fails to understand a query and returns an error, the orchestrator can either try a fallback or say *"I’m sorry, I couldn’t compute that. Maybe refine the question."*. We will surface such messages in a polite manner rather than a stack trace. Streamlit can also display warnings if the query is out of scope (like asking about a player not in the match, etc.). Because we have guardrails, many errors (like forbidden queries) will be caught early and a safe message returned by the tool (e.g., "Cannot perform that action"). The orchestrator can then relay or adjust that for the user.

- **Observability & Logging:** For development and possibly for the user to trust the answer, we may log the steps taken. If enabled (perhaps in a debug mode), the app could show an accordion with *"Show Analysis Steps"* that reveals the sequence of actions (which tools were called, any SQL executed, etc.). This is similar to a reasoning trace. For example, it could list: *"Step1: SQL query executed to find top DPS player", "Step2: Result = PlayerX", "Step3: SQL query executed to get PlayerX damage over time", "Step4: Chart generated"*. This is optional but aligns with the idea of observability. The Agents SDK can provide a trace of calls, which we can transform into a human-readable log. For caching, if certain steps repeat, we might note that a cached result was used to speed up the response.

In summary, Streamlit provides the interactive front-end to our multi-agent system, handling input, output formatting, and ensuring each session’s state (the loaded data and context) is isolated. The integration aims for a smooth user experience where the complex multi-agent processing happens behind the scenes, and the user simply experiences a responsive Q&A chatbot that can discuss and visualize their game data.

## Prompt Engineering for PandasAI and LLM Agents

To ensure high-quality analysis suited for esports analysts or coaches, we employ careful **prompt engineering** for both the PandasAI tool and the agents’ system messages. Some strategies:

- **Domain Context in Prompts:** All agents and PandasAI are instructed with context about **MOBA (SMITE) gameplay** and the data schema. For example, the PandasAI prompt extension might include: *“The data is from a SMITE 2 match combat log. 'Damage' is measured per event, players have roles (carry, support, etc.), the timeline includes objectives like Gold Fury or Fire Giant. Provide insights with this context in mind.”* This helps the LLM not to misinterpret columns or ask irrelevant questions. We use table schema info (column names and meanings) as part of the agent instructions or even directly in the prompt for PandasAI if needed. Knowing the schema ensures the LLM-generated pandas code uses correct column names.

- **Style and Clarity:** Since our audience is analysts/coaches, prompts encourage **concise, insight-focused answers**. We instruct agents to avoid overly technical language for the user-facing answer; instead use esports terminology and clear explanations. For example, rather than a raw statement *“PlayerX: damage_per_min=500”*, the agent should say *“Player X dealt about 500 damage per minute, indicating a very high pressure on the enemy.”*. We incorporate such guidance in the system prompt and few-shot examples, so the agent’s tone is explanatory and professional. Bullet points might be used for listing insights (“- **PlayerX**: 25.4 DPS (highest in match)\n- **PlayerY**: 20.1 DPS...”) because analysts appreciate at-a-glance stats.

- **Disambiguation:** If a user’s question is vague or could mean multiple things, the Orchestrator agent is prompted to ask a clarifying question rather than guess (or it can make an assumption but state it). For instance, *“highest DPS”* could mean overall or at a specific phase; the agent might clarify *“Do you mean overall average DPS or during a certain period?”* if not specified. This prevents wasted analysis on the wrong interpretation. The Agents SDK allows multi-turn, so the Orchestrator can output a clarifying question to the user and wait for input.

- **PandasAI Query Framing:** We craft PandasAI prompts to be specific and testable. PandasAI works best when the question is clearly defined. We therefore programmatically generate PandasAI prompts from the agent’s needs. For example, if an agent wants to compare two tables, it might form: *“Using DataFrame1 (combat events) and DataFrame2 (reward events), find any correlation between team fights and gold swings. Specifically: list the timestamps of team fights (sequential kills) and the gold differences before and after.”* This instructs PandasAI exactly what to do. We avoid open-ended or ambiguous language in these prompts. We also indicate the desired output format if needed: e.g., *“return a DataFrame with columns: fight_time, kills_in_fight, gold_change”*. This guides PandasAI to produce structured results.

- **Agent Few-Shot Examples:** In the Orchestrator’s system prompt, we can include a few example Q&A pairs (demonstrations) showing how to break down a query. For instance: *User: "Which team controlled the objectives?" -> Orchestrator: calls TimelineAgent -> TimelineAgent returns summary -> Orchestrator formats answer highlighting objective control.* Including such examples can train the orchestrator to follow the intended process. Similarly, specialized agents might get example prompts: *“Q: Who got First Blood and at what time? A: (calls SQL) … (answers with player name and timestamp)”* to ground their behavior.

- **Iterative Re-prompting:** If PandasAI returns code that doesn’t execute (perhaps the LLM wrote something incorrect), our tool can catch the exception and automatically adjust the prompt. For example, if PandasAI tries to use a column name that doesn’t exist (maybe a slight typo or singular vs plural mismatch), we detect the error message, and our `run_pandasai_prompt` could modify the prompt to correct the column name or provide the LLM with the correct schema and retry. Another strategy is to include the schema in the initial prompt to PandasAI as a reference (like: *“Columns available: player_name, damage, timestamp, ...”* so it doesn’t guess wrong). This kind of prompt refinement loop can significantly improve success rates.

- **Visualization Prompts:** If we do allow PandasAI to create charts (via code), we instruct it to use matplotlib in a non-blocking way. For instance, *“If a visualization is helpful, you may use matplotlib to plot, but do not call plt.show(); instead just return the figure or data.”* However, as noted, we prefer to separate chart generation, so typically PandasAI prompts will focus on data, not directly plotting. If an agent explicitly wants PandasAI to suggest a chart, it might just retrieve the data and then the orchestrator calls `generate_chart`. We guide agents to prefer that separation: “When you have data that could be visualized, obtain the data then use the chart tool.”

- **Emphasizing Quality and Relevance:** All prompts remind the agents to verify that the answer addresses the user’s question. For example, end the system prompt with: *“Double-check that your final answer is relevant and fully answers the question. If multiple steps were involved, ensure the final output compiles all necessary information.”* This reduces the chance of the agent forgetting part of the query. Also, *“If providing a chart or table, also summarize its key insight in text.”* – we want the answer not to just dump data but interpret it (as a human analyst would). This is important for coaches who may want not just the numbers but an explanation.

By engineering the prompts with domain context, examples, and clear instructions, the system will produce more reliable and insightful analysis. PandasAI becomes a powerful ally for computation when given well-defined tasks, and the LLM agents remain focused and factual, mirroring the style an esports coach would expect (highlighting pivotal moments, top performers, weaknesses, etc., based on the data).

## Iteration, Validation, and Refinement Strategies

To build a reliable system, the agents will not always accept the first result blindly. They incorporate a few strategies to **evaluate and iterate on intermediate results**:

- **Result Validation:** After a tool returns data, the agent (Orchestrator or specialist) evaluates if it makes sense. This can be done via:
  - **Sanity checks in code:** e.g., if a query was supposed to return a single row but returned none, or if a value is outside expected bounds (like negative damage or a timestamp beyond match duration), the tool can flag this or the agent can notice. The Orchestrator might have rules like *“if no results, maybe the query was too narrow or the log didn’t record that – consider altering approach.”* 
  - **LLM evaluation:** The agent’s chain-of-thought can include a step: *“Observation: the result is X. Is this likely correct?”* The LLM might know, for instance, if asked for a top player and got a blank, that’s not right, so it should try a different method.
  - **Cross-checking:** For critical answers, the system can double-check via a different route. E.g., if PandasAI computed a statistic, the Orchestrator might run an SQL query to verify that number. Or if two different tools can compute the same thing, do both and compare. This ensures consistency and catches tool errors.

- **Iterative Query Refinement:** If an initial attempt is unsatisfactory, the agents can refine it:
  - **Adjusting SQL:** Maybe the first SQL was too naive. E.g., to find “peak damage moment”, the agent first tried grouping per minute. If multiple players had separate spikes, it might refine by focusing on the top player’s events only. It can add conditions or change grouping interval (per 30 seconds instead of per minute) and re-run.
  - **Refining PandasAI Prompt:** If PandasAI returned a very verbose answer when we wanted a table, the agent can re-prompt it with *“Only give the data table, no explanation.”*. Conversely, if PandasAI gave just numbers but the question asks for insight, the agent might ask PandasAI *“explain the significance of these results.”* or use the numbers itself to compose an explanation.
  - **Handling Chart Complexity:** If the data for a chart is too granular (e.g., hundreds of data points making the chart cluttered), the orchestrator can decide to simplify. For instance, if a timeline of every kill is too busy, it might aggregate kills per 5-minute interval before plotting, or limit to a specific phase of the game. It could instruct `generate_chart` to focus on a subset (perhaps via the spec parameter, like "last 20 minutes only"). If the first chart attempt is not clear, the orchestrator might add annotations or switch chart type (say the agent first made a line chart but a bar chart might present the data more clearly – it can call generate_chart again with `chart_type="bar"`). This iterative chart refinement can be guided by simple rules or an LLM prompt: *“Does this chart effectively answer the question? If not, consider a different visualization.”*.

- **Use of Temporary Tables/DataFrames:** To facilitate iteration, the system often stores intermediate results. For example, after computing DPS for all players, keeping that in a DataFrame `dps_df` means if the user’s next question is *“Who had the second highest DPS?"*, we don’t need to recompute – the agent can just look at `dps_df`. We isolate such intermediate data by marking it as temporary and not mixing with original data (to avoid confusion). The orchestrator may assign names like `temp1`, `temp2` for each step’s output. If a subsequent step fails or needs to backtrack, having these saved allows re-use without re-running expensive operations.

- **Logging and Learning from Interactions:** If the system logs each question and how it was answered (including any missteps and corrections), developers can later analyze this to improve prompts or add new rules. Over time, common patterns of failure can be addressed. For example, if we notice PandasAI often struggles with a particular type of question, we might handle that type with a direct SQL approach or enhance the prompt with more clues. This continuous refinement loop ensures the system becomes more robust.

- **Safety Iteration:** We also consider cases where the agent might produce an answer that, while data-driven, could be sensitive or need redaction (not likely in game data, but just in case). If a result inadvertently contained something like a player’s personal info (not applicable here, since it’s all game data), a guardrail would stop it. The agent would then either drop that part of the answer or generalize it. Similarly, if a user tries to get the agent to do something outside the allowed scope (like ask it to run arbitrary code via the tools), the system should refuse. We include safety instructions to the LLM (as system content) to prevent misuse of tools (the LLM should not attempt a disallowed function call due to those instructions combined with function availability limited to our provided ones).

- **Final Answer Check:** Before displaying, the orchestrator’s last step can be a quick self-review. The answer could be passed through a final formatting function to ensure all parts are present (for example, if a chart was supposed to be included but the path is missing, insert it, or if citations needed, ensure they are formatted). The LLM could also be prompted at the end: *“Is the answer complete and easy to understand? If yes, finalize. If not, improve it.”* – though this might lead to additional minor OpenAI calls, it can polish responses.

By integrating these iterative strategies, the system behaves more like an analyst that double-checks their work. The goal is to deliver **accurate, relevant, and well-presented answers**. For example, when analyzing combat data, the system might initially identify a spike in damage at 15:00 minute mark; upon review it realizes that was due to a single event (Fire Giant fight), so it adds that context: “(during a Fire Giant fight)” in the answer. These kinds of refinements differentiate a raw data dump from a truly insightful analysis.

## Conclusion and Future Enhancements

In this technical design, we have outlined a robust multi-agent chatbot that uses OpenAI’s latest Agents API capabilities and PandasAI to enable conversational data analysis of SMITE 2 combat logs. The architecture balances the power of direct database queries with the flexibility of LLM-driven Pandas analysis, orchestrated through a triage agent that can delegate tasks to specialized agents. The system produces rich answers combining text, tables, and charts, turning raw match data into meaningful insights. Key design points like tool modularity, careful prompt engineering, and iterative refinement ensure the answers are not only correct but also tailored to the needs of esports analysts, presented in an intuitive Streamlit interface.

Going forward, this system can be extended with more agents (for example, a web search agent to fetch meta stats or a coaching tips agent to suggest improvements based on data), or even integrate audio (using OpenAI’s voice tools for spoken questions/answers). The use of the Agents SDK means the solution is maintainable and scalable – new tools or agents can be added by defining a Python function and instruction, and the framework handles the rest. With logging and user feedback, the prompts and heuristics can be continuously refined, leading to smarter agents over time.

In summary, this specification provides a blueprint for a state-of-the-art AI assistant for SMITE 2 data analytics, harnessing OpenAI’s multi-agent orchestration and the PandasAI’s data comprehension ([Pandas-ai](https://pandasai-docs.readthedocs.io/#:~:text=makes%20Pandas%20conversational%2C%20allowing%20you,DataFrame%20containing%20only%20those%20rows)) to deliver a seamless Q&A experience that feels like conversing with a knowledgeable data analyst who has full command of the underlying numbers. This will empower coaches and players to query their game performance and learn from the data in real-time, simply by asking questions.